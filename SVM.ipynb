{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkg9z5BrU6Et"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "In this notebook we will be using SVM model for the **detection of human values** given a specifc argument. It consists of a **Multilabel text classification** problem where a given piece of text needs to be classified into one or more categories out of the given list. For example, as in this case, a given argument can be categorized into 1 or more human values.\n",
        "\n",
        "#### Flow of the notebook\n",
        "\n",
        "The notebook will be divided into seperate sections to provide a organized walk through for the process used. The sections are:\n",
        "\n",
        "1. [Importing Python Libraries and preparing the environment](#section01)\n",
        "2. [Importing and Pre-Processing the domain data](#section02)\n",
        "3. [Text preprocessing](#section03)\n",
        "4. [Tf-idf vectorization](#section04)\n",
        "5. [Building Pipeline](#section05)\n",
        "6. [Training](#section06)\n",
        "7. [Evaluation](#section07)\n",
        "8. [Results](#section08)\n",
        "\n",
        "#### Technical Details\n",
        "\n",
        " - Data:\n",
        "\t - We are using the data available on [Zenodo](https://zenodo.org/record/7550385#.Y8wMquzMK3I)\n",
        "     - [Human Value Detection 2023](https://touche.webis.de/semeval23/touche23-web/index.html) is the competion which provide the souce dataset\n",
        "\t - We are referring only to the following data: `arguments-training.tsv`, `arguments-validation`, `labels-training.tsv`, `labels-validation.tsv`\n",
        "\n",
        "     - `arguments-<split>.tsv`: Each row corresponds to one argument\n",
        "        - `Argument ID`: The unique identifier for the argument\n",
        "        - `Conclusion`: Conclusion text of the argument\n",
        "        - `Stance`: Stance of the Premise towards the Conclusion; one of \"in favor of\", \"against\"\n",
        "        - `Premise`: Premise text of the argument\n",
        "\n",
        "     - `labels-<split>.tsv`: Each row corresponds to one argument\n",
        "        - `Argument ID`: The unique identifier for the argument\n",
        "        - `Other`: Each other column corresponds to one value category, with a 1 meaning that the argument resorts to the value category and a 0 that not\n",
        "---\n",
        "***NOTE***\n",
        "- *Since test data are provided without labels, we did not consider them for our analysis. In this regards **the performances of our models have been tested only on validation data**.*\n",
        "---\n",
        "\n",
        " - Script Objective:\n",
        "\t - Builiding SVM model such that: given a textual argument, classify whether or not the argument draws on one of the following categories:\n",
        "        * Self-direction: thought      \n",
        "        * Self-direction: action       \n",
        "        * Stimulation     \n",
        "        * Hedonism       \n",
        "        * Achievement      \n",
        "        * Power: dominance       \n",
        "        * Power: resources       \n",
        "        * Face       \n",
        "        * Security: personal      \n",
        "        * Security: societal      \n",
        "        * Tradition       \n",
        "        * Conformity: rules       \n",
        "        * Conformity: interpersonal       \n",
        "        * Humility       \n",
        "        * Benevolence: caring       \n",
        "        * Benevolence: dependability      \n",
        "        * Universalism: concern       \n",
        "        * Universalism: nature       \n",
        "        * Universalism: tolerance      \n",
        "        * Universalism: objectivity       \n",
        "\n",
        "---\n",
        "***NOTE***\n",
        "- *It is to be noted that the overall mechanisms for a multiclass and multilabel problems are similar, except for few differences namely:*\n",
        "\t- *Loss function is designed to evaluate all the probability of categories individually rather than as compared to other categories. Hence the use of `BCE` rather than `Cross Entropy` when defining loss.*\n",
        "\t- *Sigmoid of the outputs calcuated to rather than Softmax. Again for the reasons defined in the previous point*\n",
        "\t- *The [accuracy metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) and [F1 scores](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) used from sklearn package as compared to direct comparison of expected vs predicted*\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suW29itLWEGs"
      },
      "source": [
        "<a id='section01'></a>\n",
        "### Importing Python Libraries and preparing the environment\n",
        "\n",
        "At this step we will be importing the libraries and modules needed to run our script. Libraries are:\n",
        "* Numpy\n",
        "* Pandas\n",
        "* Seaborn\n",
        "* Matplotlib\n",
        "* Sci-kit learn\n",
        "\n",
        "RANDOM SEED has been settled to 42 to ensure experiments reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8eQ1mVxc3ev",
        "outputId": "fc779cb6-f6cd-4402-d646-7a914847fb63"
      },
      "outputs": [],
      "source": [
        "!pip install mlcm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NRZtRrPkuq-g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import math\n",
        "from mlcm import mlcm\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UwitSnabntXQ"
      },
      "outputs": [],
      "source": [
        "# Random seed to repeat experiments.\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsiCjVZWWZYI"
      },
      "source": [
        "<a id='section02'></a>\n",
        "### Loading data\n",
        "\n",
        "We will be working with the data and preparing for fine tuning purposes, *assuming that the `arguments-train.tsv`, `labels-train.tsv`, `arguments-validation.tsv`, `labels-validation.tsv` are already downloaded and saved in our `data` folder*\n",
        "\n",
        "* Import files in different dataframes\n",
        "* Merge arguments and labels dataframe of each corresponding split\n",
        "* Taking the values of all the categories and coverting it into a list.\n",
        "* Considering only Premises as text input for our classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FffmFH3RwY5b",
        "outputId": "23f73021-d6f8-441b-e5ee-123428d598b5"
      },
      "outputs": [],
      "source": [
        "args_train = pd.read_csv(r\"C:\\Users\\Praveen\\Desktop\\SEM-6\\DL\\dataset-1\\training-english\\sentences.tsv\", delimiter='\\t')\n",
        "labels_train = pd.read_csv(r\"C:\\Users\\Praveen\\Desktop\\SEM-6\\DL\\dataset-1\\training-english\\labels.tsv\", delimiter='\\t')\n",
        "labels_train.replace(0.5, 0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "d0JzPB4gw61I",
        "outputId": "969ae013-1137-4ec3-88f6-fc13cb625482"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text-ID</th>\n",
              "      <th>Sentence-ID</th>\n",
              "      <th>Self-direction: thought attained</th>\n",
              "      <th>Self-direction: thought constrained</th>\n",
              "      <th>Self-direction: action attained</th>\n",
              "      <th>Self-direction: action constrained</th>\n",
              "      <th>Stimulation attained</th>\n",
              "      <th>Stimulation constrained</th>\n",
              "      <th>Hedonism attained</th>\n",
              "      <th>Hedonism constrained</th>\n",
              "      <th>...</th>\n",
              "      <th>Benevolence: caring attained</th>\n",
              "      <th>Benevolence: caring constrained</th>\n",
              "      <th>Benevolence: dependability attained</th>\n",
              "      <th>Benevolence: dependability constrained</th>\n",
              "      <th>Universalism: concern attained</th>\n",
              "      <th>Universalism: concern constrained</th>\n",
              "      <th>Universalism: nature attained</th>\n",
              "      <th>Universalism: nature constrained</th>\n",
              "      <th>Universalism: tolerance attained</th>\n",
              "      <th>Universalism: tolerance constrained</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BG_002</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BG_002</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BG_002</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BG_002</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BG_002</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 40 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  Text-ID  Sentence-ID  Self-direction: thought attained  \\\n",
              "0  BG_002            1                               0.0   \n",
              "1  BG_002            2                               0.0   \n",
              "2  BG_002            3                               0.0   \n",
              "3  BG_002            4                               0.0   \n",
              "4  BG_002            5                               0.0   \n",
              "\n",
              "   Self-direction: thought constrained  Self-direction: action attained  \\\n",
              "0                                  0.0                              0.0   \n",
              "1                                  0.0                              0.0   \n",
              "2                                  0.0                              0.0   \n",
              "3                                  0.0                              0.0   \n",
              "4                                  0.0                              0.0   \n",
              "\n",
              "   Self-direction: action constrained  Stimulation attained  \\\n",
              "0                                 0.0                   0.0   \n",
              "1                                 0.0                   0.0   \n",
              "2                                 0.0                   0.0   \n",
              "3                                 0.0                   0.0   \n",
              "4                                 0.0                   0.0   \n",
              "\n",
              "   Stimulation constrained  Hedonism attained  Hedonism constrained  ...  \\\n",
              "0                      0.0                0.0                   0.0  ...   \n",
              "1                      0.0                0.0                   0.0  ...   \n",
              "2                      0.0                1.0                   0.0  ...   \n",
              "3                      0.0                0.0                   0.0  ...   \n",
              "4                      0.0                0.0                   0.0  ...   \n",
              "\n",
              "   Benevolence: caring attained  Benevolence: caring constrained  \\\n",
              "0                           0.0                              0.0   \n",
              "1                           0.0                              0.0   \n",
              "2                           0.0                              0.0   \n",
              "3                           0.0                              0.0   \n",
              "4                           0.0                              0.0   \n",
              "\n",
              "   Benevolence: dependability attained  \\\n",
              "0                                  0.0   \n",
              "1                                  0.0   \n",
              "2                                  0.0   \n",
              "3                                  0.0   \n",
              "4                                  0.0   \n",
              "\n",
              "   Benevolence: dependability constrained  Universalism: concern attained  \\\n",
              "0                                     0.0                             0.0   \n",
              "1                                     0.0                             0.0   \n",
              "2                                     0.0                             0.0   \n",
              "3                                     0.0                             0.0   \n",
              "4                                     0.0                             0.0   \n",
              "\n",
              "   Universalism: concern constrained  Universalism: nature attained  \\\n",
              "0                                0.0                            0.0   \n",
              "1                                0.0                            0.0   \n",
              "2                                0.0                            0.0   \n",
              "3                                0.0                            0.0   \n",
              "4                                0.0                            0.0   \n",
              "\n",
              "   Universalism: nature constrained  Universalism: tolerance attained  \\\n",
              "0                               0.0                               0.0   \n",
              "1                               0.0                               0.0   \n",
              "2                               0.0                               0.0   \n",
              "3                               0.0                               0.0   \n",
              "4                               0.0                               0.0   \n",
              "\n",
              "   Universalism: tolerance constrained  \n",
              "0                                  0.0  \n",
              "1                                  0.0  \n",
              "2                                  0.0  \n",
              "3                                  0.0  \n",
              "4                                  0.0  \n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GyxxvJihxGV1"
      },
      "outputs": [],
      "source": [
        "args_test = pd.read_csv(r\"C:\\Users\\Praveen\\Desktop\\SEM-6\\DL\\dataset-1\\validation-english\\sentences.tsv\", delimiter='\\t')\n",
        "labels_test = pd.read_csv(r\"C:\\Users\\Praveen\\Desktop\\SEM-6\\DL\\dataset-1\\validation-english\\labels.tsv\", delimiter='\\t')\n",
        "labels_test.replace(0.5, 0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EwktYOzPxSH_",
        "outputId": "328cf646-9a54-49c5-c354-961026a8422a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text-ID</th>\n",
              "      <th>Sentence-ID</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EN_003</td>\n",
              "      <td>1</td>\n",
              "      <td>PM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EN_003</td>\n",
              "      <td>2</td>\n",
              "      <td>Orban: Gradual reopening may begin after Easter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EN_003</td>\n",
              "      <td>3</td>\n",
              "      <td>The faster we vaccinate, the sooner the number...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EN_003</td>\n",
              "      <td>4</td>\n",
              "      <td>This is why it is important not to be deceived...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EN_003</td>\n",
              "      <td>5</td>\n",
              "      <td>The prime minister said he expects that the nu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Text-ID  Sentence-ID                                               Text\n",
              "0  EN_003            1                                                 PM\n",
              "1  EN_003            2    Orban: Gradual reopening may begin after Easter\n",
              "2  EN_003            3  The faster we vaccinate, the sooner the number...\n",
              "3  EN_003            4  This is why it is important not to be deceived...\n",
              "4  EN_003            5  The prime minister said he expects that the nu..."
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "args_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6J38wuvxYXd",
        "outputId": "773d0cc0-6ab7-48c1-fc6e-26d4e7553fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(14904, 40)\n",
            "(44758, 40)\n"
          ]
        }
      ],
      "source": [
        "print(labels_test.shape)\n",
        "print(labels_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZmZ9U2HF1q"
      },
      "source": [
        "#SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5FErkznvWCd"
      },
      "source": [
        "<a id='section03'></a>\n",
        "# Text Preprocessing with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmSOUyd_5b__",
        "outputId": "67d16ba8-5b25-4dda-e696-52c7da7f6d55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Praveen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# import these modules\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "#REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "PUNTUACTIONS = string.punctuation\n",
        "BAD_SYMBOLS_RE = re.compile('[^a-z ]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "\n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = text.lower() # lowercase text\n",
        "    #text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in PUNTUACTIONS) # delete stopwors from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    text = \" \".join(lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niV1oCH3Jy3a"
      },
      "source": [
        "'Conclusion' is not taken into account since it doesn't add any additional information, it is merely a repetition of what have been said in 'premise'. This also prevent overfitting as we are easing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vrJYN-bPLh1N"
      },
      "outputs": [],
      "source": [
        "train = args_train.copy()\n",
        "train['preprocessed'] = args_train['Text'].apply(lambda text: preprocess(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qRmvBy3l71HF",
        "outputId": "e363ff3c-1516-4b73-b6ad-32b823cb6595"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hispanic voter losing faith democratic party poll'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['preprocessed'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aChUX5e_3Imp"
      },
      "outputs": [],
      "source": [
        "test = args_test.copy()\n",
        "test['preprocessed'] = args_test['Text'].apply(lambda text: preprocess(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text-ID</th>\n",
              "      <th>Sentence-ID</th>\n",
              "      <th>Text</th>\n",
              "      <th>preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>1</td>\n",
              "      <td>Hispanic Voters Are Losing Faith In The Democr...</td>\n",
              "      <td>hispanic voter losing faith democratic party poll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>2</td>\n",
              "      <td>The support of Hispanic voters at the midterms...</td>\n",
              "      <td>support hispanic voter midterm later year coul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>3</td>\n",
              "      <td>U.S. President Joe Biden speaks to employees a...</td>\n",
              "      <td>u president joe biden speaks employee lockheed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>4</td>\n",
              "      <td>(Julie Bennett/Getty Images) According to a Qu...</td>\n",
              "      <td>julie bennettgetty image according quinnipiac ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>5</td>\n",
              "      <td>This marks the lowest approval rating of any d...</td>\n",
              "      <td>mark lowest approval rating demographic group</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44753</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>12</td>\n",
              "      <td>The rent economy, which provides easy profits ...</td>\n",
              "      <td>rent economy provides easy profit provide empl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44754</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>13</td>\n",
              "      <td>The tax base will be broadened, the tax burden...</td>\n",
              "      <td>tax base broadened tax burden poor eased trans...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44755</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>14</td>\n",
              "      <td>Supportive arrangements will be made to increa...</td>\n",
              "      <td>supportive arrangement made increase social we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44756</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>15</td>\n",
              "      <td>In order to lift out of poverty those who are ...</td>\n",
              "      <td>order lift poverty affected poverty low level ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44757</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>16</td>\n",
              "      <td>Basic needs, education, health and social serv...</td>\n",
              "      <td>basic need education health social service mad...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44758 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Text-ID  Sentence-ID  \\\n",
              "0        EN_001            1   \n",
              "1        EN_001            2   \n",
              "2        EN_001            3   \n",
              "3        EN_001            4   \n",
              "4        EN_001            5   \n",
              "...         ...          ...   \n",
              "44753  TR_M_022           12   \n",
              "44754  TR_M_022           13   \n",
              "44755  TR_M_022           14   \n",
              "44756  TR_M_022           15   \n",
              "44757  TR_M_022           16   \n",
              "\n",
              "                                                    Text  \\\n",
              "0      Hispanic Voters Are Losing Faith In The Democr...   \n",
              "1      The support of Hispanic voters at the midterms...   \n",
              "2      U.S. President Joe Biden speaks to employees a...   \n",
              "3      (Julie Bennett/Getty Images) According to a Qu...   \n",
              "4      This marks the lowest approval rating of any d...   \n",
              "...                                                  ...   \n",
              "44753  The rent economy, which provides easy profits ...   \n",
              "44754  The tax base will be broadened, the tax burden...   \n",
              "44755  Supportive arrangements will be made to increa...   \n",
              "44756  In order to lift out of poverty those who are ...   \n",
              "44757  Basic needs, education, health and social serv...   \n",
              "\n",
              "                                            preprocessed  \n",
              "0      hispanic voter losing faith democratic party poll  \n",
              "1      support hispanic voter midterm later year coul...  \n",
              "2      u president joe biden speaks employee lockheed...  \n",
              "3      julie bennettgetty image according quinnipiac ...  \n",
              "4          mark lowest approval rating demographic group  \n",
              "...                                                  ...  \n",
              "44753  rent economy provides easy profit provide empl...  \n",
              "44754  tax base broadened tax burden poor eased trans...  \n",
              "44755  supportive arrangement made increase social we...  \n",
              "44756  order lift poverty affected poverty low level ...  \n",
              "44757  basic need education health social service mad...  \n",
              "\n",
              "[44758 rows x 4 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "dfcTc0ck2hyB",
        "outputId": "f61c5818-20bb-466f-bef9-2bc0273547db"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text-ID</th>\n",
              "      <th>preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>hispanic voter losing faith democratic party poll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>support hispanic voter midterm later year coul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>u president joe biden speaks employee lockheed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>julie bennettgetty image according quinnipiac ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>mark lowest approval rating demographic group</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44753</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>rent economy provides easy profit provide empl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44754</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>tax base broadened tax burden poor eased trans...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44755</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>supportive arrangement made increase social we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44756</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>order lift poverty affected poverty low level ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44757</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>basic need education health social service mad...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44758 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Text-ID                                       preprocessed\n",
              "0        EN_001  hispanic voter losing faith democratic party poll\n",
              "1        EN_001  support hispanic voter midterm later year coul...\n",
              "2        EN_001  u president joe biden speaks employee lockheed...\n",
              "3        EN_001  julie bennettgetty image according quinnipiac ...\n",
              "4        EN_001      mark lowest approval rating demographic group\n",
              "...         ...                                                ...\n",
              "44753  TR_M_022  rent economy provides easy profit provide empl...\n",
              "44754  TR_M_022  tax base broadened tax burden poor eased trans...\n",
              "44755  TR_M_022  supportive arrangement made increase social we...\n",
              "44756  TR_M_022  order lift poverty affected poverty low level ...\n",
              "44757  TR_M_022  basic need education health social service mad...\n",
              "\n",
              "[44758 rows x 2 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = train.drop(columns= train.columns[1:3])\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "uURRfJOe3Tn2",
        "outputId": "4621c2a4-5988-46fb-ed92-fa59ba2790f9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text-ID</th>\n",
              "      <th>preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EN_003</td>\n",
              "      <td>pm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EN_003</td>\n",
              "      <td>orban gradual reopening may begin easter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EN_003</td>\n",
              "      <td>faster vaccinate sooner number infection case ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EN_003</td>\n",
              "      <td>important deceived promote antivaccination vie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EN_003</td>\n",
              "      <td>prime minister said expects number vaccinated ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14899</th>\n",
              "      <td>TR_M_024</td>\n",
              "      <td>quality lowcost energy supply realized</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14900</th>\n",
              "      <td>TR_M_024</td>\n",
              "      <td>public energy investment realized planned stab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14901</th>\n",
              "      <td>TR_M_024</td>\n",
              "      <td>public enterprise engaged exploration extracti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14902</th>\n",
              "      <td>TR_M_024</td>\n",
              "      <td>energy planning international agreement securi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14903</th>\n",
              "      <td>TR_M_024</td>\n",
              "      <td>biofuel production emphasized investment energ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14904 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Text-ID                                       preprocessed\n",
              "0        EN_003                                                 pm\n",
              "1        EN_003           orban gradual reopening may begin easter\n",
              "2        EN_003  faster vaccinate sooner number infection case ...\n",
              "3        EN_003  important deceived promote antivaccination vie...\n",
              "4        EN_003  prime minister said expects number vaccinated ...\n",
              "...         ...                                                ...\n",
              "14899  TR_M_024             quality lowcost energy supply realized\n",
              "14900  TR_M_024  public energy investment realized planned stab...\n",
              "14901  TR_M_024  public enterprise engaged exploration extracti...\n",
              "14902  TR_M_024  energy planning international agreement securi...\n",
              "14903  TR_M_024  biofuel production emphasized investment energ...\n",
              "\n",
              "[14904 rows x 2 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = test.drop(columns= test.columns[1:3])\n",
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text-ID</th>\n",
              "      <th>preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>hispanic voter losing faith democratic party poll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>support hispanic voter midterm later year coul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>u president joe biden speaks employee lockheed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>julie bennettgetty image according quinnipiac ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EN_001</td>\n",
              "      <td>mark lowest approval rating demographic group</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44753</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>rent economy provides easy profit provide empl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44754</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>tax base broadened tax burden poor eased trans...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44755</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>supportive arrangement made increase social we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44756</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>order lift poverty affected poverty low level ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44757</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>basic need education health social service mad...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44758 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Text-ID                                       preprocessed\n",
              "0        EN_001  hispanic voter losing faith democratic party poll\n",
              "1        EN_001  support hispanic voter midterm later year coul...\n",
              "2        EN_001  u president joe biden speaks employee lockheed...\n",
              "3        EN_001  julie bennettgetty image according quinnipiac ...\n",
              "4        EN_001      mark lowest approval rating demographic group\n",
              "...         ...                                                ...\n",
              "44753  TR_M_022  rent economy provides easy profit provide empl...\n",
              "44754  TR_M_022  tax base broadened tax burden poor eased trans...\n",
              "44755  TR_M_022  supportive arrangement made increase social we...\n",
              "44756  TR_M_022  order lift poverty affected poverty low level ...\n",
              "44757  TR_M_022  basic need education health social service mad...\n",
              "\n",
              "[44758 rows x 2 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#train.drop(columns=['Sentence-ID'], inplace=True)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text-ID</th>\n",
              "      <th>Self-direction: thought attained</th>\n",
              "      <th>Self-direction: thought constrained</th>\n",
              "      <th>Self-direction: action attained</th>\n",
              "      <th>Self-direction: action constrained</th>\n",
              "      <th>Stimulation attained</th>\n",
              "      <th>Stimulation constrained</th>\n",
              "      <th>Hedonism attained</th>\n",
              "      <th>Hedonism constrained</th>\n",
              "      <th>Achievement attained</th>\n",
              "      <th>...</th>\n",
              "      <th>Benevolence: caring attained</th>\n",
              "      <th>Benevolence: caring constrained</th>\n",
              "      <th>Benevolence: dependability attained</th>\n",
              "      <th>Benevolence: dependability constrained</th>\n",
              "      <th>Universalism: concern attained</th>\n",
              "      <th>Universalism: concern constrained</th>\n",
              "      <th>Universalism: nature attained</th>\n",
              "      <th>Universalism: nature constrained</th>\n",
              "      <th>Universalism: tolerance attained</th>\n",
              "      <th>Universalism: tolerance constrained</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BG_002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BG_002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BG_002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BG_002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BG_002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44753</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44754</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44755</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44756</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44757</th>\n",
              "      <td>TR_M_022</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44758 rows × 39 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Text-ID  Self-direction: thought attained  \\\n",
              "0        BG_002                               0.0   \n",
              "1        BG_002                               0.0   \n",
              "2        BG_002                               0.0   \n",
              "3        BG_002                               0.0   \n",
              "4        BG_002                               0.0   \n",
              "...         ...                               ...   \n",
              "44753  TR_M_022                               0.0   \n",
              "44754  TR_M_022                               0.0   \n",
              "44755  TR_M_022                               0.0   \n",
              "44756  TR_M_022                               0.0   \n",
              "44757  TR_M_022                               0.0   \n",
              "\n",
              "       Self-direction: thought constrained  Self-direction: action attained  \\\n",
              "0                                      0.0                              0.0   \n",
              "1                                      0.0                              0.0   \n",
              "2                                      0.0                              0.0   \n",
              "3                                      0.0                              0.0   \n",
              "4                                      0.0                              0.0   \n",
              "...                                    ...                              ...   \n",
              "44753                                  0.0                              0.0   \n",
              "44754                                  0.0                              0.0   \n",
              "44755                                  0.0                              0.0   \n",
              "44756                                  0.0                              1.0   \n",
              "44757                                  0.0                              0.0   \n",
              "\n",
              "       Self-direction: action constrained  Stimulation attained  \\\n",
              "0                                     0.0                   0.0   \n",
              "1                                     0.0                   0.0   \n",
              "2                                     0.0                   0.0   \n",
              "3                                     0.0                   0.0   \n",
              "4                                     0.0                   0.0   \n",
              "...                                   ...                   ...   \n",
              "44753                                 0.0                   0.0   \n",
              "44754                                 0.0                   0.0   \n",
              "44755                                 0.0                   0.0   \n",
              "44756                                 0.0                   0.0   \n",
              "44757                                 0.0                   0.0   \n",
              "\n",
              "       Stimulation constrained  Hedonism attained  Hedonism constrained  \\\n",
              "0                          0.0                0.0                   0.0   \n",
              "1                          0.0                0.0                   0.0   \n",
              "2                          0.0                1.0                   0.0   \n",
              "3                          0.0                0.0                   0.0   \n",
              "4                          0.0                0.0                   0.0   \n",
              "...                        ...                ...                   ...   \n",
              "44753                      0.0                0.0                   0.0   \n",
              "44754                      0.0                0.0                   0.0   \n",
              "44755                      0.0                0.0                   0.0   \n",
              "44756                      0.0                0.0                   0.0   \n",
              "44757                      0.0                0.0                   0.0   \n",
              "\n",
              "       Achievement attained  ...  Benevolence: caring attained  \\\n",
              "0                       0.0  ...                           0.0   \n",
              "1                       0.0  ...                           0.0   \n",
              "2                       0.0  ...                           0.0   \n",
              "3                       0.0  ...                           0.0   \n",
              "4                       0.0  ...                           0.0   \n",
              "...                     ...  ...                           ...   \n",
              "44753                   0.0  ...                           0.0   \n",
              "44754                   0.0  ...                           1.0   \n",
              "44755                   1.0  ...                           0.0   \n",
              "44756                   0.0  ...                           1.0   \n",
              "44757                   0.0  ...                           1.0   \n",
              "\n",
              "       Benevolence: caring constrained  Benevolence: dependability attained  \\\n",
              "0                                  0.0                                  0.0   \n",
              "1                                  0.0                                  0.0   \n",
              "2                                  0.0                                  0.0   \n",
              "3                                  0.0                                  0.0   \n",
              "4                                  0.0                                  0.0   \n",
              "...                                ...                                  ...   \n",
              "44753                              0.0                                  0.0   \n",
              "44754                              0.0                                  0.0   \n",
              "44755                              0.0                                  0.0   \n",
              "44756                              0.0                                  0.0   \n",
              "44757                              0.0                                  0.0   \n",
              "\n",
              "       Benevolence: dependability constrained  Universalism: concern attained  \\\n",
              "0                                         0.0                             0.0   \n",
              "1                                         0.0                             0.0   \n",
              "2                                         0.0                             0.0   \n",
              "3                                         0.0                             0.0   \n",
              "4                                         0.0                             0.0   \n",
              "...                                       ...                             ...   \n",
              "44753                                     0.0                             0.0   \n",
              "44754                                     0.0                             0.0   \n",
              "44755                                     0.0                             0.0   \n",
              "44756                                     0.0                             0.0   \n",
              "44757                                     0.0                             0.0   \n",
              "\n",
              "       Universalism: concern constrained  Universalism: nature attained  \\\n",
              "0                                    0.0                            0.0   \n",
              "1                                    0.0                            0.0   \n",
              "2                                    0.0                            0.0   \n",
              "3                                    0.0                            0.0   \n",
              "4                                    0.0                            0.0   \n",
              "...                                  ...                            ...   \n",
              "44753                                0.0                            0.0   \n",
              "44754                                0.0                            0.0   \n",
              "44755                                0.0                            0.0   \n",
              "44756                                0.0                            0.0   \n",
              "44757                                0.0                            0.0   \n",
              "\n",
              "       Universalism: nature constrained  Universalism: tolerance attained  \\\n",
              "0                                   0.0                               0.0   \n",
              "1                                   0.0                               0.0   \n",
              "2                                   0.0                               0.0   \n",
              "3                                   0.0                               0.0   \n",
              "4                                   0.0                               0.0   \n",
              "...                                 ...                               ...   \n",
              "44753                               0.0                               0.0   \n",
              "44754                               0.0                               0.0   \n",
              "44755                               0.0                               0.0   \n",
              "44756                               0.0                               0.0   \n",
              "44757                               0.0                               0.0   \n",
              "\n",
              "       Universalism: tolerance constrained  \n",
              "0                                      0.0  \n",
              "1                                      0.0  \n",
              "2                                      0.0  \n",
              "3                                      0.0  \n",
              "4                                      0.0  \n",
              "...                                    ...  \n",
              "44753                                  0.0  \n",
              "44754                                  0.0  \n",
              "44755                                  0.0  \n",
              "44756                                  0.0  \n",
              "44757                                  0.0  \n",
              "\n",
              "[44758 rows x 39 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_train.drop(columns=['Sentence-ID'], inplace=True)\n",
        "labels_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rrd7oLjJMT9E",
        "outputId": "3ca4fa43-3aff-418e-868a-1b266f4ad53f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessed_text</th>\n",
              "      <th>list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hispanic voter losing faith democratic party poll</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>support hispanic voter midterm later year coul...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u president joe biden speaks employee lockheed...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>julie bennettgetty image according quinnipiac ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mark lowest approval rating demographic group</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44753</th>\n",
              "      <td>rent economy provides easy profit provide empl...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44754</th>\n",
              "      <td>tax base broadened tax burden poor eased trans...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44755</th>\n",
              "      <td>supportive arrangement made increase social we...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44756</th>\n",
              "      <td>order lift poverty affected poverty low level ...</td>\n",
              "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44757</th>\n",
              "      <td>basic need education health social service mad...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>44758 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       preprocessed_text  \\\n",
              "0      hispanic voter losing faith democratic party poll   \n",
              "1      support hispanic voter midterm later year coul...   \n",
              "2      u president joe biden speaks employee lockheed...   \n",
              "3      julie bennettgetty image according quinnipiac ...   \n",
              "4          mark lowest approval rating demographic group   \n",
              "...                                                  ...   \n",
              "44753  rent economy provides easy profit provide empl...   \n",
              "44754  tax base broadened tax burden poor eased trans...   \n",
              "44755  supportive arrangement made increase social we...   \n",
              "44756  order lift poverty affected poverty low level ...   \n",
              "44757  basic need education health social service mad...   \n",
              "\n",
              "                                                    list  \n",
              "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
              "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "...                                                  ...  \n",
              "44753  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "44754  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "44755  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
              "44756  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "44757  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "\n",
              "[44758 rows x 2 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = labels_train.copy()\n",
        "df['list'] = df[df.columns[1:]].values.tolist()\n",
        "df_train = train[[\"preprocessed\"]].copy()\n",
        "feature_name = \"preprocessed_text\"\n",
        "df_train.columns = [feature_name]\n",
        "df_train['list'] = df['list']\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38\n"
          ]
        }
      ],
      "source": [
        "print(len(df_train.iloc[0].list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jyqCAXsN3YNl",
        "outputId": "20ff649d-0fad-419f-9ff5-a4442683737a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessed_text</th>\n",
              "      <th>list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pm</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>orban gradual reopening may begin easter</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>faster vaccinate sooner number infection case ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>important deceived promote antivaccination vie...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>prime minister said expects number vaccinated ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14899</th>\n",
              "      <td>quality lowcost energy supply realized</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14900</th>\n",
              "      <td>public energy investment realized planned stab...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14901</th>\n",
              "      <td>public enterprise engaged exploration extracti...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14902</th>\n",
              "      <td>energy planning international agreement securi...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14903</th>\n",
              "      <td>biofuel production emphasized investment energ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14904 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       preprocessed_text  \\\n",
              "0                                                     pm   \n",
              "1               orban gradual reopening may begin easter   \n",
              "2      faster vaccinate sooner number infection case ...   \n",
              "3      important deceived promote antivaccination vie...   \n",
              "4      prime minister said expects number vaccinated ...   \n",
              "...                                                  ...   \n",
              "14899             quality lowcost energy supply realized   \n",
              "14900  public energy investment realized planned stab...   \n",
              "14901  public enterprise engaged exploration extracti...   \n",
              "14902  energy planning international agreement securi...   \n",
              "14903  biofuel production emphasized investment energ...   \n",
              "\n",
              "                                                    list  \n",
              "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "...                                                  ...  \n",
              "14899  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "14900  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "14901  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "14902  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "14903  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
              "\n",
              "[14904 rows x 2 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = labels_test.copy()\n",
        "df['list'] = df[df.columns[2:]].values.tolist()\n",
        "df_test = test[[\"preprocessed\"]].copy()\n",
        "feature_name = \"preprocessed_text\"\n",
        "df_test.columns = [feature_name]\n",
        "df_test['list'] = df['list']\n",
        "df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qscvMxtMZuY"
      },
      "source": [
        "# TF-IDF VECTORIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38\n"
          ]
        }
      ],
      "source": [
        "print(len(df_test.iloc[0].list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uJwkjjXmM3VX"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC, SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eymLbcE7347G",
        "outputId": "9176fa73-a097-4b18-a9e6-57df943aa8cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "categories = labels_train.columns[1:]\n",
        "len(categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ivr0uZXTgbx4"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train = train.preprocessed, df_train.list\n",
        "X_test, Y_test = test.preprocessed, df_test.list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "aDmNOQDTgfg6"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train = np.stack(X_train), np.stack(Y_train)\n",
        "X_test, Y_test = np.stack(X_test), np.stack(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Assuming X_train, Y_train, X_test, Y_test are your original datasets\n",
        "\n",
        "# # Calculate the number of samples to select for the 10% subset\n",
        "# subset_size_train = int(0.1 * len(X_train))\n",
        "# subset_size_test = int(0.1 * len(X_test))\n",
        "\n",
        "# # Randomly select 10% of the data\n",
        "# subset_indices_train = np.random.choice(len(X_train), subset_size_train, replace=False)\n",
        "# subset_indices_test = np.random.choice(len(X_test), subset_size_test, replace=False)\n",
        "\n",
        "# # Create subsets\n",
        "# X_train , Y_train = X_train[subset_indices_train], Y_train[subset_indices_train]\n",
        "# X_test , Y_test = X_test[subset_indices_test], Y_test[subset_indices_test]\n",
        "\n",
        "# X_train, Y_train = np.stack(X_train), np.stack(Y_train)\n",
        "# X_test, Y_test = np.stack(X_test), np.stack(Y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1Ka4XQlghyv",
        "outputId": "4e5e9ade-592e-4bdc-ba40-ffa39bac5be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X train: (44758,) \n",
            "Y train: (44758, 38) \n",
            "\n",
            "X test: (14904,) \n",
            "Y test: (14904, 38) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"X train:\", X_train.shape, \"\\nY train:\", Y_train.shape, \"\\n\")\n",
        "print(\"X test:\", X_test.shape, \"\\nY test:\", Y_test.shape, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXyFhfInK2W7"
      },
      "source": [
        "Definition of useful variables and functions for Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "icrrVugr14pj"
      },
      "outputs": [],
      "source": [
        "# test_res = args_test.copy()\n",
        "# test_res = test_res.merge(labels_test)\n",
        "# test_res['preprocessed_premise'] = args_test['Premise'].apply(lambda text: preprocess(text))\n",
        "# test_res['preprocessed_conclusion'] = args_test['Conclusion'].apply(lambda text: preprocess(text))\n",
        "# test_res = test_res.drop(columns= test_res.columns[1:4])\n",
        "# test_res.columns\n",
        "\n",
        "# labels_res = list(labels_test.columns[1:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmrIFtl5LFw7"
      },
      "source": [
        "TfidfComparison compute a dataframe of size `labels x frequency` that shows the TF-IDF score for each label, considering the most frequent word for a selected label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "QUsdpqO0wyHA"
      },
      "outputs": [],
      "source": [
        "def TfidfComparison(dataset, label, labels, frequency=15):\n",
        "\n",
        "  # selection of the 15 most frequent word for class 'label', conclusion and premises have already been preprocessed\n",
        "  ds_copy1 = dataset.copy()\n",
        "  ds_copy1['concat'] = ds_copy1['preprocessed_conclusion'] + ' ' + ds_copy1['preprocessed_premise']\n",
        "  ds_copy1 = list(ds_copy1[ds_copy1[label]==1]['concat'])\n",
        "  words_freq = Counter([k for k in re.sub('[\\W]', ' ', ' '.join(ds_copy1).lower()).split(' ')])\n",
        "  words_freq_dict = {}\n",
        "  for element, count in words_freq.items():\n",
        "    words_freq_dict[element]=count\n",
        "\n",
        "  # sorting the words by frequency and computation of the Document Frequency for each word. The DF is computed class wise due to unbalanced number of data among the classes\n",
        "  sorted_words_freq_dict = sorted(words_freq_dict.items(), key=lambda x: x[1], reverse=True)[:frequency]\n",
        "  most_frequent_words = [w for w, v in sorted_words_freq_dict] # stored the most X words for the actual attribute\n",
        "  df = {}\n",
        "  for word in most_frequent_words:\n",
        "    df[word] = 0\n",
        "    for l in labels:\n",
        "      ds = dataset.copy()\n",
        "      ds['concat'] = ds['preprocessed_conclusion'] + ' ' + ds['preprocessed_premise']\n",
        "      ds = list(ds[ds[l]==1]['concat'])\n",
        "      label_words = list(set([k for k in re.sub('[\\W]', ' ', ' '.join(ds).lower()).split(' ')]))\n",
        "      if word in label_words:\n",
        "        df[word] += 1\n",
        "\n",
        "\n",
        "\n",
        "  # computation of TF-IDF for each class among the previously selected 15 words\n",
        "  tfidf_rows = []\n",
        "  for l in labels:\n",
        "    dict_tmp = {}\n",
        "    if l == label:\n",
        "      dict_tmp['ColumnName'] = l\n",
        "      for word, frequency in sorted_words_freq_dict:\n",
        "        try:\n",
        "          dict_tmp[word] = abs(words_freq_dict[word]/sum(list(words_freq_dict.values())) * math.log(len(labels)/df[word],10))\n",
        "        except:\n",
        "          dict_tmp[word] = 0\n",
        "      tfidf_rows.append(dict_tmp)\n",
        "      continue\n",
        "    ds_copy2 = dataset.copy()\n",
        "    ds_copy2['concat'] = ds_copy2['preprocessed_conclusion'] + ' ' + ds_copy2['preprocessed_premise']\n",
        "    ds_copy2 = list(ds_copy2[(ds_copy2[label]==0) & (ds_copy2[l]==1)]['concat'])\n",
        "    words_freq_l = Counter([k for k in re.sub('[\\W]', ' ', ' '.join(ds_copy2).lower()).split(' ')])\n",
        "    words_frequency = {}\n",
        "    for element, count in words_freq_l.items():\n",
        "      words_frequency[element]=count\n",
        "    dict_tmp['ColumnName'] = l\n",
        "    for word, frequency in sorted_words_freq_dict:\n",
        "      try:\n",
        "        dict_tmp[word] = abs(words_frequency[word]/sum(list(words_frequency.values())) * math.log(len(labels)/df[word],10))\n",
        "      except Exception as ex:\n",
        "        dict_tmp[word] = 0\n",
        "    tfidf_rows.append(dict_tmp)\n",
        "    dataframe = pd.DataFrame(tfidf_rows)\n",
        "  return dataframe.style.background_gradient(cmap='Reds', subset=dataframe.columns[1:])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC0HHG4wM6mi"
      },
      "source": [
        "# Model Definition and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8QUpQelyoo6",
        "outputId": "98f275fb-177b-44b1-a2f6-819d76a953e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vector', TfidfVectorizer(max_features=10000)),\n",
              "                ('svd', TruncatedSVD(n_components=600)),\n",
              "                ('clf',\n",
              "                 OneVsRestClassifier(estimator=SVC(C=18,\n",
              "                                                   class_weight='balanced',\n",
              "                                                   gamma=0.01, max_iter=1000000,\n",
              "                                                   random_state=42),\n",
              "                                     n_jobs=1))])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "SVC_pipeline = Pipeline([\n",
        "                ('vector', TfidfVectorizer(max_features=10000)),\n",
        "                ('svd', TruncatedSVD(n_components=600)),\n",
        "                ('clf', OneVsRestClassifier(SVC(C=18, kernel='rbf', gamma = 0.01,class_weight='balanced', max_iter=1000000, random_state=42), n_jobs=1))\n",
        "                #('clf', OneVsRestClassifier(SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, class_weight = 'balanced', random_state=42, max_iter=10000), n_jobs=1)),\n",
        "            ])\n",
        "\n",
        "SVC_pipeline.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVM157J3NCdO"
      },
      "source": [
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOqjR9CiN1GF",
        "outputId": "3de32318-c160-4d9a-eb83-676d2e0dfb2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                        precision    recall  f1-score   support\n",
            "\n",
            "      Self-direction: thought attained       0.03      0.85      0.06       447\n",
            "   Self-direction: thought constrained       0.03      0.96      0.06        80\n",
            "       Self-direction: action attained       0.06      0.68      0.11      1256\n",
            "    Self-direction: action constrained       0.02      0.93      0.05       261\n",
            "                  Stimulation attained       0.05      0.73      0.10       971\n",
            "               Stimulation constrained       0.03      0.96      0.05       126\n",
            "                     Hedonism attained       0.03      0.95      0.06       264\n",
            "                  Hedonism constrained       0.03      0.93      0.05       107\n",
            "                  Achievement attained       0.08      0.66      0.15      1925\n",
            "               Achievement constrained       0.05      0.76      0.09       797\n",
            "             Power: dominance attained       0.07      0.68      0.13      1487\n",
            "          Power: dominance constrained       0.03      0.90      0.05       338\n",
            "             Power: resources attained       0.07      0.69      0.13      1252\n",
            "          Power: resources constrained       0.05      0.75      0.09       720\n",
            "                         Face attained       0.03      0.87      0.05       380\n",
            "                      Face constrained       0.03      0.92      0.05       361\n",
            "           Security: personal attained       0.03      0.90      0.05       292\n",
            "        Security: personal constrained       0.04      0.83      0.08       561\n",
            "           Security: societal attained       0.07      0.63      0.13      1574\n",
            "        Security: societal constrained       0.11      0.63      0.19      2179\n",
            "                    Tradition attained       0.04      0.88      0.07       416\n",
            "                 Tradition constrained       0.03      0.94      0.05        85\n",
            "            Conformity: rules attained       0.08      0.67      0.15      1505\n",
            "         Conformity: rules constrained       0.05      0.74      0.09       926\n",
            "    Conformity: interpersonal attained       0.02      0.92      0.04       250\n",
            " Conformity: interpersonal constrained       0.02      0.92      0.05       319\n",
            "                     Humility attained       0.03      0.96      0.06        75\n",
            "                  Humility constrained       0.03      1.00      0.06        31\n",
            "          Benevolence: caring attained       0.05      0.74      0.10       846\n",
            "       Benevolence: caring constrained       0.03      0.97      0.06       111\n",
            "   Benevolence: dependability attained       0.04      0.80      0.08       650\n",
            "Benevolence: dependability constrained       0.03      0.96      0.05       180\n",
            "        Universalism: concern attained       0.07      0.67      0.13      1385\n",
            "     Universalism: concern constrained       0.04      0.77      0.08       736\n",
            "         Universalism: nature attained       0.06      0.80      0.11       622\n",
            "      Universalism: nature constrained       0.03      0.94      0.05       237\n",
            "      Universalism: tolerance attained       0.03      0.96      0.05       216\n",
            "   Universalism: tolerance constrained       0.02      0.95      0.05       237\n",
            "\n",
            "                             micro avg       0.05      0.74      0.09     24205\n",
            "                             macro avg       0.04      0.84      0.08     24205\n",
            "                          weighted avg       0.06      0.74      0.11     24205\n",
            "                           samples avg       0.07      0.87      0.10     24205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TRAIN\n",
        "prediction_train = SVC_pipeline.predict(X_train)\n",
        "print(classification_report(Y_train, prediction_train, target_names = categories, zero_division=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkzKNEeSXh1k",
        "outputId": "3c7a1544-c57d-4de8-baac-d01607ce4c8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                        precision    recall  f1-score   support\n",
            "\n",
            "      Self-direction: thought attained       0.01      0.36      0.02       133\n",
            "   Self-direction: thought constrained       0.00      0.03      0.00        29\n",
            "       Self-direction: action attained       0.03      0.43      0.06       370\n",
            "    Self-direction: action constrained       0.01      0.28      0.02       100\n",
            "                  Stimulation attained       0.04      0.47      0.07       366\n",
            "               Stimulation constrained       0.00      0.11      0.00        36\n",
            "                     Hedonism attained       0.01      0.31      0.02        72\n",
            "                  Hedonism constrained       0.00      0.19      0.01        27\n",
            "                  Achievement attained       0.06      0.44      0.10       676\n",
            "               Achievement constrained       0.02      0.42      0.04       227\n",
            "             Power: dominance attained       0.05      0.53      0.09       440\n",
            "          Power: dominance constrained       0.01      0.40      0.02       119\n",
            "             Power: resources attained       0.06      0.49      0.11       460\n",
            "          Power: resources constrained       0.02      0.43      0.04       188\n",
            "                         Face attained       0.01      0.33      0.02       120\n",
            "                      Face constrained       0.02      0.49      0.03       139\n",
            "           Security: personal attained       0.01      0.36      0.02        77\n",
            "        Security: personal constrained       0.02      0.43      0.04       188\n",
            "           Security: societal attained       0.05      0.42      0.09       509\n",
            "        Security: societal constrained       0.08      0.48      0.14       698\n",
            "                    Tradition attained       0.03      0.43      0.05       225\n",
            "                 Tradition constrained       0.00      0.06      0.00        35\n",
            "            Conformity: rules attained       0.07      0.52      0.12       538\n",
            "         Conformity: rules constrained       0.03      0.48      0.06       321\n",
            "    Conformity: interpersonal attained       0.00      0.23      0.01        82\n",
            " Conformity: interpersonal constrained       0.01      0.29      0.02       112\n",
            "                     Humility attained       0.01      0.15      0.01        34\n",
            "                  Humility constrained       0.00      0.00      0.00         9\n",
            "          Benevolence: caring attained       0.03      0.42      0.06       289\n",
            "       Benevolence: caring constrained       0.00      0.10      0.01        39\n",
            "   Benevolence: dependability attained       0.02      0.41      0.04       231\n",
            "Benevolence: dependability constrained       0.00      0.12      0.01        49\n",
            "        Universalism: concern attained       0.05      0.48      0.09       455\n",
            "     Universalism: concern constrained       0.02      0.36      0.03       184\n",
            "         Universalism: nature attained       0.04      0.45      0.07       233\n",
            "      Universalism: nature constrained       0.02      0.39      0.04       132\n",
            "      Universalism: tolerance attained       0.01      0.26      0.01        57\n",
            "   Universalism: tolerance constrained       0.01      0.36      0.01        58\n",
            "\n",
            "                             micro avg       0.03      0.43      0.05      8057\n",
            "                             macro avg       0.02      0.34      0.04      8057\n",
            "                          weighted avg       0.04      0.43      0.07      8057\n",
            "                           samples avg       0.06      0.73      0.07      8057\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TEST\n",
        "prediction = SVC_pipeline.predict(X_test)\n",
        "print(classification_report(Y_test, prediction, target_names = categories, zero_division=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj1bKZzUNDzF"
      },
      "source": [
        "# Error Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpwruUZTdfM8"
      },
      "source": [
        "Here, we want to know which classes have been confused with other classes and how frequently. To do this, we are going to use a Multi Label Confusion Matrix, rendered with seaborn's heatmap. For example, we can see that `Universalism: nature` has low values, since the model achieve to distiguish from the others pretty well. (confirmed also in TF-IDF analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iPHiK34sdrc3",
        "outputId": "c437fe8b-2e7b-4d63-bda3-842f57a43832"
      },
      "outputs": [],
      "source": [
        "conf_mat,normal_conf_mat = mlcm.cm(prediction,Y_test)\n",
        "df_cm = pd.DataFrame(conf_mat, index = [i for i in categories.append(pd.Index(['noclass']))],\n",
        "                  columns = [i for i in categories.append(pd.Index(['noclass']))])\n",
        "plt.figure(figsize = (20,14))\n",
        "sn.heatmap(df_cm, annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj8SBpmNddYF"
      },
      "source": [
        "We retained senseful to analyse the TF-IDF scores that words obtained across all the labels, since SVM is based on this. In general we observed that low support usually coincide with low F1-score, but some anomalies caught our attention.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YxYATimSrTz"
      },
      "source": [
        "`Conformity: interpersonal` obtain the lowest F1-score(0.1) and it also has the lowest support(60). The matrix shows perfectly that the most frequent word for this label usually are confused with many other labels(they obtain a good tf-idf score even among other labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "xk_OV55vNFrv",
        "outputId": "71d17040-cf5a-4f01-ed63-38f397be7b60"
      },
      "outputs": [],
      "source": [
        "TfidfComparison(test_res, 'Conformity: interpersonal', labels_res, frequency=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRnOiwiSURN3"
      },
      "source": [
        "`Universalism: nature` show low support(127) but it achieve one of the better F1 score(0.46). As can be seen on the matrix, words have a strong TF-IDF score only for `Universalism: nature`, making them good identifiers for this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "YmrUNhVjPIPT",
        "outputId": "95aca59a-93f9-42a6-b552-0535f9883090"
      },
      "outputs": [],
      "source": [
        "TfidfComparison(test_res, 'Universalism: nature', labels_res, frequency=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMSxuDlhU3Kf"
      },
      "source": [
        "`Universalism: tolerance` besides having a good support(223) achieves not so good results(0.26). This because its most frequent words are important also for other labels, making them indistinguishable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "RbZlLxfrQlHE",
        "outputId": "19372d84-63c4-482e-f212-cb5b22482ddf"
      },
      "outputs": [],
      "source": [
        "TfidfComparison(test_res, 'Universalism: tolerance', labels_res, frequency=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkpzjjTNr4mc"
      },
      "source": [
        "`Security: personal`: high support and high F1 score. Here we can see that its frequent words are good identifiers since the tf-idf scores are high and different from other classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiesOTmur4mc",
        "outputId": "f799243b-d291-424d-c540-db40e9ad41c8"
      },
      "outputs": [],
      "source": [
        "TfidfComparison(test_res, 'Security: personal', labels_res, frequency=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsCsUGzar4md"
      },
      "source": [
        "`Benevolence: dependability`: we can see that `Security: personal` also shares high TF-IDF for frequent words. It can be seen in the multi label confusion matrix that the `Benevolence: dependability` it is usually misclassified as this one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWSw2rkfr4md",
        "outputId": "a592b8e2-0e57-471b-e733-22709758fec5"
      },
      "outputs": [],
      "source": [
        "TfidfComparison(test_res, 'Benevolence: dependability', labels_res, frequency=15)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
